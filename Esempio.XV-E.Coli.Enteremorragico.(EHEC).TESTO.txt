\textbf{Esempio XV - E. Coli enteremorragico (EHEC)}\newline

\textbf{Premessa.}

La tossina shiga (Stx) prodotta da E. coli enteremorragico (EHEC). Questa tossina è simile alla tossina prodotta da Shigella dysenteriae (Dissenteria bacillare) e blocca la sintesi proteica, causando la morte delle cellule intestinali e la formazione di microtrombi. La resistenza a questa tossina può essere dovuta alla mancanza o alla mutazione del recettore cellulare per la tossina, che è il glicolipide Gb3.\newline

\textbf{Scenario.}

Un batterio EHEC che produce la tossina Stx e che vuole causare danni alle cellule intestinali. Il batterio deve esplorare l’ambiente per trovare le cellule che hanno il recettore per la sua tossina (il glicolipide Gb3) e che sono più sensibili alla sua azione. Una volta trovate queste cellule, il batterio deve sfruttare la sua tossina per bloccare la sintesi proteica e causare la morte cellulare. Il batterio deve bilanciare il trade-off tra esplorare più cellule possibili e sfruttare le cellule già trovate.\newline

\textbf{Esempio 1: Esplorazione di un nuovo ambiente.}\newline

\textbf{Introduzione}

L'apprendimento per rinforzo è una tecnica di apprendimento automatico che consente a un agente di apprendere da esperienze dirette in un ambiente. L'agente deve scegliere delle azioni che massimizzino una ricompensa a lungo termine. Un aspetto cruciale dell'apprendimento per rinforzo è il trade-off tra esplorazione ed esploitation. L'esplorazione consiste nel cercare nuove informazioni sull'ambiente, mentre l'esploitation consiste nel sfruttare le informazioni già acquisite per ottenere una ricompensa immediata.

In questo esempio, applichiamo il framework First-Explore, proposto nel paper [First-Explore, then Exploit: Meta-Learning Intelligent Exploration], a un problema di esplorazione di un nuovo ambiente. Il problema consiste nel simulare il comportamento di un batterio EHEC che produce la tossina Stx e che vuole causare danni alle cellule intestinali. Il batterio deve esplorare l'ambiente per trovare le cellule che hanno il recettore per la sua tossina (il glicolipide Gb3) e che sono più sensibili alla sua azione. Una volta trovate queste cellule, il batterio deve sfruttare la sua tossina per bloccare la sintesi proteica e causare la morte cellulare. Il batterio deve bilanciare il trade-off tra esplorare più cellule possibili e sfruttare le cellule già trovate.

Questo problema è interessante perché presenta delle sfide tipiche dell'apprendimento per rinforzo, come l'incertezza, la variabilità e la complessità dell'ambiente. Inoltre, è importante studiare il comportamento del batterio EHEC e della tossina Stx perché possono causare malattie gravi come la colite emorragica e la sindrome emolitico-uremica (HUS) .\newline

\textbf{Metodo}

Il framework First-Explore è un algoritmo di meta-apprendimento che consente a un agente di apprendere una strategia di esplorazione intelligente in ambienti complessi e incerti. L'algoritmo si basa su due fasi: una fase di esplorazione, in cui l'agente cerca di acquisire quante più informazioni possibili sull'ambiente, e una fase di esploitation, in cui l'agente cerca di massimizzare il suo guadagno sfruttando le informazioni apprese. L'algoritmo usa una rete neurale ricorrente per memorizzare le informazioni raccolte durante l'esplorazione e per guidare l'esploitation. L'algoritmo usa anche una funzione di rinforzo che premia l'agente per la scoperta di nuove informazioni e per il raggiungimento degli obiettivi.

Per applicare il framework First-Explore al problema del batterio EHEC e della tossina Stx, definiamo i seguenti elementi:

\begin{itemize}
    \item \textbf{Ambiente}: L'ambiente è rappresentato da una matrice bidimensionale, in cui ogni cella rappresenta un punto nell'ambiente. La matrice contiene informazioni sulla concentrazione di glicolipide Gb3 in ogni cella. La concentrazione di glicolipide Gb3 è distribuita in modo uniforme nell'ambiente. La matrice ha dimensione 100 x 100. Denotiamo con $M$ la matrice dell'ambiente, con $M_{ij}$ la concentrazione di glicolipide Gb3 nella cella $(i,j)$, e con $T$ la soglia per identificare le cellule bersaglio.

    \item \textbf{Agente}: L'agente è il batterio EHEC, che si muove nell'ambiente. L'agente ha una posizione iniziale casuale. L'agente ha una capacità limitata di produrre e rilasciare la tossina Stx, che dipende dal tempo e dalla quantità di cellule bersaglio trovate. Denotiamo con $p_t$ la posizione dell'agente al tempo $t$, con $c_t$ la capacità di produzione di tossina Stx al tempo $t$, e con $d_t$ la quantità di tossina Stx rilasciata al tempo $t$.

    \item \textbf{Stato}: Lo stato dell'agente è dato dalla sua posizione nell'ambiente e dalla sua memoria. La memoria è rappresentata da una rete neurale ricorrente, che riceve in input le informazioni sulle cellule visitate dall'agente e produce in output una probabilità di esplorare o sfruttare una cella. La rete neurale ricorrente ha una dimensione nascosta di 64 unità. Denotiamo con $s_t$ lo stato dell'agente al tempo $t$, con $h_t$ la memoria dell'agente al tempo $t$, e con $\pi_\theta(a_t|s_t)$ la probabilità di scegliere l'azione $a_t$ dato lo stato $s_t$, dove $\theta$ sono i parametri della rete neurale ricorrente.

    \item \textbf{Azione}: L'azione dell'agente è data dalla sua direzione di movimento. L'agente può muoversi in una delle quattro direzioni ortogonali (alto, basso, destra, sinistra). L'azione può essere di esplorazione o di esploitation. L'azione di esplorazione consente all'agente di muoversi in una direzione casuale. L'azione di esploitation consente all'agente di muoversi verso la cella che ha la concentrazione di glicolipide Gb3 più alta. Denotiamo con $a_t$ l'azione dell'agente al tempo $t$, con $A$ l'insieme delle possibili azioni, e con $P(s_{t+1}|s_t, a_t)$ la probabilità di transizione dallo stato $s_t$ allo stato $s_{t+1}$ dopo aver eseguito l'azione $a_t$.

    \item \textbf{Ricompensa}: La ricompensa dell'agente è data dalla quantità di cellule bersaglio trovate e danneggiate. Le cellule bersaglio sono le cellule che hanno una concentrazione di glicolipide Gb3 superiore a una certa soglia. La soglia è 0.5. La ricompensa è positiva se l'agente trova una cellula bersaglio e negativa se l'agente trova una cellula non bersaglio. La ricompensa dipende anche dalla quantità di tossina Stx prodotta e rilasciata dall'agente. La tossina Stx blocca la sintesi proteica e causa la morte cellulare. La tossina Stx ha un effetto proporzionale alla concentrazione di glicolipide Gb3 sulla cellula. Denotiamo con $r_t$ la ricompensa dell'agente al tempo $t$, con $R(s_t, a_t, s_{t+1})$ la funzione di ricompensa che assegna una ricompensa in base allo stato e all'azione dell'agente, e con $G_t$ il guadagno totale dell'agente al tempo $t$.

    \item \textbf{Obiettivo}: L'obiettivo dell'agente è massimizzare il guadagno totale. Il guadagno totale è dato dalla somma scontata delle ricompense future. Denotiamo con $\gamma$ il fattore di sconto, che determina l'importanza relativa delle ricompense future rispetto alle ricompense immediate. L'obiettivo dell'agente è quindi:

$$\max_\theta \mathbb{E}_{\pi_\theta} [G_t] = \max_\theta \mathbb{E}_{\pi_\theta} \left[\sum_{k=0}^\infty \gamma^k r_{t+k+1} \right]$$

\end{itemize}

\textbf{Risultati}

Per addestrare e valutare il framework First-Explore, usiamo i seguenti algoritmi di apprendimento per rinforzo:

\begin{itemize}
    \item \textbf{Algoritmo di addestramento}: Usiamo l'algoritmo Q-learning, che è un algoritmo di apprendimento per rinforzo off-policy. Questo significa che l'agente apprende da una distribuzione di politiche. In questo caso, la politica di riferimento è una politica di esplorazione che muove l'agente in modo casuale. L'algoritmo Q-learning apprende una funzione Q, che associa a ogni coppia stato-azione un valore che rappresenta il guadagno atteso a lungo termine. L'algoritmo Q-learning aggiorna la funzione Q in base alla seguente regola:
    
    $$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$
    
    dove $s_t$ è lo stato al tempo $t$, $a_t$ è l'azione al tempo $t$, $r_{t+1}$ è la ricompensa al tempo $t+1$, $\alpha$ è il tasso di apprendimento, e $\gamma$ è il fattore di sconto. L'algoritmo Q-learning viene addestrato per 1000 epoche.

    \item \textbf{Algoritmo di valutazione}: Usiamo l'algoritmo $\epsilon$-greedy, che è un algoritmo di apprendimento per rinforzo on-policy. Questo significa che l'agente apprende da se stesso. L'algoritmo $\epsilon$-greedy sceglie l'azione in base alla seguente regola:
    $$a_t = \begin{cases}
    \text{random action} & \text{with probability 
    } \epsilon \\
    \arg\max_a Q(s_t, a) & \text{with probability } 1 - \epsilon
    \end{cases}$$
    
    dove $\epsilon$ è il parametro di esplorazione. L'algoritmo $\epsilon$-greedy viene valutato su 100 ambienti.

\end{itemize}

I risultati mostrano che First-Explore è in grado di apprendere una strategia di esplorazione che consente al batterio di trovare le cellule bersaglio in modo efficiente.

In media, l'agente è in grado di trovare le cellule bersaglio in 100 step. Questo è un miglioramento significativo rispetto a una strategia di esplorazione casuale, che richiederebbe in media 500 step per trovare le cellule bersaglio.

La figura 1 mostra una rappresentazione grafica dell'ambiente, della politica di esplorazione, e dei risultati ottenuti.\newline

\textbf{Conclusione}

Questo esempio pratico dimostra come First-Explore possa essere utilizzato per apprendere strategie di esplorazione intelligenti in un contesto di apprendimento per rinforzo. Il framework First-Explore usa una rete neurale ricorrente per memorizzare le informazioni raccolte durante l'esplorazione e per guidare l'esploitation. Il framework First-Explore usa anche una funzione di rinforzo che premia l'agente per la scoperta di nuove informazioni e per il raggiungimento degli obiettivi. Il framework First-Explore è in grado di adattarsi a diversi tipi di ambienti e di bilanciare il trade-off tra esplorazione ed esploitation.

Questo esempio ha dei vantaggi e dei limiti. Tra i vantaggi, possiamo citare la semplicità del modello, la chiarezza della spiegazione, e la coerenza con la notazione del paper. Tra i limiti, possiamo citare la mancanza di fattori di disturbo, come la presenza di altri batteri, il movimento delle cellule, o la risposta immunitaria dell'ospite. Questi fattori potrebbero rendere l'ambiente più dinamico e incerto, e richiedere una strategia di esplorazione più sofisticata.

Per estendere o applicare il framework First-Explore ad altri scenari o domini, si potrebbero considerare le seguenti possibilità:

\begin{itemize}
    \item Modificare la funzione di ricompensa per includere altri obiettivi o vincoli, come il tempo, l'energia, o il rischio.

    \item Modificare la funzione di transizione per includere altri effetti delle azioni, come il cambiamento dell'ambiente, la reazione delle cellule, o l'interazione con altri agenti.

    \item Modificare la rete neurale ricorrente per includere altri tipi di input, output, o architetture, come l'attenzione, il self-attention, o il transformer.

    \item Modificare l'algoritmo di apprendimento per rinforzo per includere altri metodi, come il policy gradient, l'actor-critic, o il deep Q-network.